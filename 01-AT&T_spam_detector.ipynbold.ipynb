{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M08-deep-learning/AT%26T_logo_2016.svg\" alt=\"AT&T LOGO\" width=\"50%\" />\n",
    "\n",
    "# Orange SPAM detector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company's Description üìá\n",
    "\n",
    "AT&T Inc. is an American multinational telecommunications holding company headquartered at Whitacre Tower in Downtown Dallas, Texas. It is the world's largest telecommunications company by revenue and the third largest provider of mobile telephone services in the U.S. As of 2022, AT&T was ranked 13th on the Fortune 500 rankings of the largest United States corporations, with revenues of $168.8 billion! üòÆ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project üöß\n",
    "\n",
    "One of the main pain point that AT&T users are facing is constant exposure to SPAM messages.\n",
    "\n",
    "AT&T has been able to manually flag spam messages for a time, but they are looking for an automated way of detecting spams to protect their users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals üéØ\n",
    "\n",
    "Your goal is to build a spam detector, that can automatically flag spams as they come based solely on the sms' content."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of this project üñºÔ∏è\n",
    "\n",
    "To start off, AT&T would like you to use the folowing dataset:\n",
    "\n",
    "[Dowload the Dataset](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/project/spam.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Data\n",
    "%pwd\n",
    "!wget https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/project/spam.csv\n",
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers ü¶Æ\n",
    "\n",
    "To help you achieve this project, here are a few tips that should help you: \n",
    "\n",
    "### Start simple\n",
    "A good deep learing model does not necessarily have to be super complicated!\n",
    "\n",
    "### Transfer learning\n",
    "You do not have access to a whole lot of data, perhaps channeling the power of a more sophisticated model trained on billions of observations might help!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable üì¨\n",
    "\n",
    "To complete this project, your team should: \n",
    "\n",
    "* Write a notebook that runs preprocessing and trains one or more deep learning models in order to predict the spam or ham nature of the sms\n",
    "* State the achieved performance clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets transformers evaluate rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dataset = pd.read_csv(\"./Data/spam.csv\",encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset exploration\n",
    "\n",
    "Nous explorons le jeu de donn√©es pour comprendre sa structure par rapport au probl√®me d'analyse de spam qui nous pr√©occupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spam_dataset.head())\n",
    "display(spam_dataset.info())\n",
    "display(spam_dataset.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL sur le jeu de donn√©es de SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dataset.dropna(inplace=True)\n",
    "spam_dataset.rename(columns={ 'v1': 'type','v2': 'line1', 'Unnamed: 2': 'line2','Unnamed: 3': 'line3','Unnamed: 4': 'line4'}, inplace=True)\n",
    "spam_dataset.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous fusionnons les lignes du texto en un unique message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_dataset['message'] = spam_dataset[['line1', 'line2', 'line3', 'line4']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous transformons la colonne 'v1' en une colonne binaire 'spam' o√π 'spam' devient 1 et 'ham' devient 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "  \"spam\": 1,\n",
    "  \"ham\": 0\n",
    "}\n",
    "spam_dataset[\"spam\"] = spam_dataset[\"type\"].map(mapping)\n",
    "spam_dataset = spam_dataset[['spam', 'message','type']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification de l'int√©gration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spam_dataset.isnull().sum())\n",
    "spam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "spam_dataset = Dataset.from_pandas(spam_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing du dataset \n",
    "Il est n√©cessaire de convertir le texte en suite de nombres avec un \"tokenizer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"message\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = spam_dataset.map(tokenize_function)\n",
    "samples = tokenized_datasets[:]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jeux d'entrainement et de test\n",
    "Le jeu de donn√©es est propre et pr√™t, il faut maintenant le diviser en un jeu d'entrainement, un jeu de validation et un jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=spam_dataset.drop('spam', axis=1)\n",
    "y=spam_dataset['spam']\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.2, random_state= 42)\n",
    "\n",
    "print(\"Train set:\", x_train.shape, y_train.shape)\n",
    "print(\"Test set:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classifier = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", \n",
    "                      tokenizer=\"google-bert/bert-base-cased\")\n",
    "spam_preds = classifier(spam_train['text'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jedha-dsfs-ft-35",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
